{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8389994-3a3b-42f5-87a5-a5dcf04d06e9",
   "metadata": {},
   "source": [
    "# Chapter 4: Markov chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafd14ca-ed77-4362-a9db-494efd3cb650",
   "metadata": {},
   "source": [
    "## Markov property"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bed78c-67ce-48db-a4e2-b5ae2738d95c",
   "metadata": {},
   "source": [
    "$P(X_{n+1}|X_0=i_0, X_1=i_1...X_n=i_n)=P(X_{n+1}=j|X_n=i)$\n",
    "\n",
    "Homogeneous: $P(X_{n+1}=j|X_n=i)=P(X_i=j|X_0=i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed53bf4c-41b8-4040-88e6-71900185e33f",
   "metadata": {},
   "source": [
    "## Example 4.1 (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c04a403-0de3-410e-b33f-ebce412f90fe",
   "metadata": {},
   "source": [
    "Customers arrive at a service center according to a Poisson process with parameter $\\lambda$. There is a single server and service times are distributed according to arbitrary distribution G."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c1fa2d-42bf-4268-803f-b07ff3bce6b4",
   "metadata": {},
   "source": [
    "Solution: consider only when customers depart. Let $X_n$ be the number of customers left behind by the nth departure, and $Y_n$ be the number of customers that arrive during the (n+1)th customer.\n",
    "\n",
    "Then, when customer n leaves, there are $X_n$ customers left behind waiting, and 1 enters service. $Y_n$ customers enter according to a Poisson process during that service. So, at the next departure time, the line will contain $X_n - 1 + Y_n$ customers (or $Y_n$ if $X_n=0$).\n",
    "\n",
    "Then, $Y_n$ is distributed according to a Poisson process (because the intervals are non-overlapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdba7ab5-a371-4a1f-bce6-2272283f0b2c",
   "metadata": {},
   "source": [
    "## Basic properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12f19e4-e046-40ab-b585-37b7e73a9ccf",
   "metadata": {},
   "source": [
    "a) $P(X_0=i_0, X_1=i_1... X_n=i_n|X_0=i_0)=P_{i_0i_1}...P_{i_{n-1}i_n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ff174e-8c96-462e-9250-017085e9a67a",
   "metadata": {},
   "source": [
    "b) Time-homogeneity: $P(X_{k+1}=i_{k+1},...,X_{k+n}=i_{k+n}|X_k=i_k)=P(X_1=i_{k+1}, ... X_n=i_{k+n}|X_0=i_k)$\n",
    "\n",
    "Proof:\n",
    "\n",
    "By definition of conditional probability:\n",
    "\n",
    "$P(X_{k+1}=i_{k+1},...,X_{k+n}=i_{k+n}|X_k=i_k)=P(X_k=i_k, X_{k+1}=i_{k+1},...,X_{k+n}=i_{k+n})/P(X_k=i_k)$\n",
    "\n",
    "Expand over every possible path arriving at $X_k$:\n",
    "\n",
    "= $\\frac{\\sum_{i_0, i_1..., i_{k-1}}P(X_0=i_0...X_{k-1}=i_{k-1},X_k=i_k...X_{k+n}=i_{k+n})P(X_0=i_0)}{\\sum_{i_0, i_1..., i_{k-1}}P(X_0=i_0...X_{k-1}=i_{k-1},X_k=i_k)P(X_0=i_0)}$\n",
    "\n",
    "By property a:\n",
    "\n",
    "= $\\frac{\\sum_{i_0, i_1..., i_{k-1}}P_{i_0i_1}...P_{i_{k-1}i_k}...P_{i_{k+n-1}i_{k+n}}P(X_0=i_0)}{\\sum_{i_0, i_1..., i_{k-1}}P_{i_0i_1}...P_{i_{k-1}i_k}P(X_0=i_0)}$\n",
    "\n",
    "= $ P_{i_ki_{k+1}}...P_{i_{k+n-1}i_{k+n}} = P(X_0=i_k, X_1=i_{k+1}...X_{k+n}=i_{k+n}|X_0=i_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c427ad-aed3-42e5-b7c0-0156a7489cdd",
   "metadata": {},
   "source": [
    "## Chapman-Kolmogorov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea1ac46-147e-4893-95c0-cc9d91d6168b",
   "metadata": {},
   "source": [
    "$P_{ij}^{(n)}$ is the n-step transition probability (getting from i to j in n steps)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba0ecf8-af09-4c99-b763-beb6d8473698",
   "metadata": {},
   "source": [
    "Property: $P^{(n)}_{ij}=(P^n)_{ij}$ (the n-step transition probability between i and j can be calculated by raising the transition matrix to power n, then taking the ijth entry)\n",
    "\n",
    "Proof:\n",
    "\n",
    "$P^{(n)}_{ij}=P(X_n=j|X_0=i)=\\sum_{i_1, ...i_{n-1}}P(X_0=i, X_1=i_1...X_n=j|X_0=i_0)$\n",
    "\n",
    "$=\\sum_{i_1, ...i_{n-1}}P_{ii_1}P_{i_1i_2}...P_{i_{n-1}j}$\n",
    "\n",
    "$=(P^n)_{ij}$ (by definition of matrix multiplication)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b741a8f-95da-41f9-83ac-c016b3497ecf",
   "metadata": {},
   "source": [
    "Chapman-Kolmogorov Equation: $p^{(n+m)}=p^{(n)}p^{(m)}$ (the n+m-step transition probability can be calculated by multiplying the m and n-step transition matrices). Proof follows from last property.\n",
    "\n",
    "Additionally, $p^{(0)}=I$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48c17d5-a070-49de-b7c3-6ebcbe8a1fd5",
   "metadata": {},
   "source": [
    "Example: Let $\\mu_n(i)=P(X_n=i)$ (the probability distribution of $X_n$). Then, $\\mu_{(n+k)}=\\mu_kP^n$.\n",
    "\n",
    "Proof: $\\mu_{n+k}(j)=P(X_{n+k}=j)$ (by definition)\n",
    "\n",
    "= $\\sum_{i}P(X_{n+k}=j|X_k=i)P(X_k=i)$ (law of total probability conditioning on $X_k$)\n",
    "\n",
    "$P(X_k=i)=\\mu_k(i)$ and $P(X_{n+k}=j|X_k=i)=P^{(n)}_{ij}$, so\n",
    "\n",
    "= $\\sum_{i}P^{(n)}_{ij}\\mu_k(i)$\n",
    "\n",
    "By the definition of matrix multiplication, \n",
    "\n",
    "= $\\mu_kP^n_{ij}=\\mu_kP^{(n)}_{ij}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f1a1e5-105f-4a24-8df7-223070dd4c53",
   "metadata": {},
   "source": [
    "## Strong Markov Property"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f7cce6-e50a-40a0-a5cf-223a5a845062",
   "metadata": {},
   "source": [
    "Stopping time can be determined as a function of only past events.\n",
    "\n",
    "Markov property: probability of n+m steps can be written as product of n steps by m steps (with new condition)\n",
    "\n",
    "Strong Markov property: $E_{x_0}[g(X_0, X_1,... X_n)h(X_n, X_{n+1}...X_{n+m})]=E_{x_0}[g(X_0, X_1...X_n)E_{x_n}[h(X_0, X_{1}...X_{m}]]$\n",
    "\n",
    "Proof:\n",
    "\n",
    "$E_{x_0}[g(X_0, X_1,... X_n)h(X_n, X_{n+1}...X_{n+m})]=\\sum_{x_1...x_{n+m}}g(x_0...x_n)h(x_n...x_{n+m})p(x_0,x_1)...p(x_{n+m-1},x_{n+m})$\n",
    "\n",
    "= $\\sum_{x_1...x_n}g(x_0...x_n)p(x_0,x_1)...p(x_{n-1},x_n)\\sum_{x_{n+1}..x_{n+m}}h(x_n...x_{n+m})p(x_n,x_{n+1})...p(x_{n+m-1},x_{n+m})$\n",
    "\n",
    "= $E_{x_0}[g(X_0, X_1...X_n)E_{x_n}[h(X_0, X_{1}...X_{m}]]$\n",
    "\n",
    "Strong Markov property with stopping time:\n",
    "\n",
    "$E_{x_0}[1_{T<\\infty}g_T(x_0,...,x_T)h(x_T,...,x_{T+m})]=E_{x_0}[1_{T<\\infty}g_T(x_0,...,x_T)E_{x_T}[h(x_T,...,x_{T+m})]$\n",
    "\n",
    "Proof: $1_{T<\\infty}=\\sum_{k=0}^{\\infty}1_{T=k}$\n",
    "\n",
    "So, $E_{x_0}[1_{T<\\infty}g_T(x_0,...,x_T)h(x_T,...,x_{T+m})]=\\sum_{k=0}^{\\infty}E_{x_0}[1_{T=k}g_T(x_0,...,x_T)h(x_T,...,x_{T+m})]$ \n",
    "\n",
    "Then, when T=k, $X_{T+i}=X_{k+i}$\n",
    "\n",
    "So, = $\\sum_{k=0}^{\\infty}E_{x_0}[1_{T=k}g_k(x_0,...,x_k)h(x_k,...,x_{k+m})]$\n",
    "\n",
    "By Markov property:\n",
    "\n",
    "= $\\sum_{k=0}^{\\infty}E_{x_0}[1_{T=k}g_k(x_0,...,x_k)E_{x_k}[h(x_k,...,x_{k+m})]]$\n",
    "\n",
    "Swap the expectation and sum:\n",
    "\n",
    "= $E_{x_0}[\\sum_{k=0}^{\\infty} 1_{T=k}g_k(x_0,...,x_k)E_{x_k}[h(x_k,...,x_{k+m})]]$\n",
    "\n",
    "Use summation identity in reverse direction and replace k with random variable T:\n",
    "\n",
    "= $E_{x_0}[1_{T<\\infty}g_T(x_0,...,x_T)E_{x_T}[x_k,...,x_{T+m}]]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c743fe-3f1c-46b3-b66a-b649aa98689e",
   "metadata": {},
   "source": [
    "## Classification of states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c92068b-f2fb-4ee1-8cbc-e56bd4e88cb6",
   "metadata": {},
   "source": [
    "### Accessibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106e0817-fc99-4a90-bbf4-80194dd140be",
   "metadata": {},
   "source": [
    "j is accessible from i (i -> j), if $P^n_{ij}>0$ for some $n \\geq 0$.\n",
    "\n",
    "Properties\n",
    "\n",
    "a) $i \\rightarrow j$ iff $P_i(X_n=j,$ for some $n \\geq 0$) > 0\n",
    "\n",
    "Proof: if $i \\rightarrow j$, then there exists an $n_0$ such that $P_{ij}^{n_0}>0$. Then, $P_i(X_n=j,$ for some $n \\geq 0$)=$P_i(\\cup_{n=0}^{\\infty}(X_n=j))\\geq P_i(X_{n_0}=j)=P_{ij}^{n_0}>0$\n",
    "\n",
    "Reverse direction: Suppose $P_i(X_n=j,$ for some $n \\geq 0$) > 0.\n",
    "\n",
    "Then, $P_i(X_n=j,$ for some $n \\geq 0$) = $P_i(\\cup_{n=0}^{\\infty}(X_n=j)) \\leq \\sum_{n=0}^{\\infty}P_i(X_n=j) > 0$, so there must exist an n such that $P_i(X_n=j)>0$. So, there exists an n such that $P_{ij}^n>0$, so $i \\rightarrow j$.\n",
    "\n",
    "b) if i -> j, then either i = j or there exists a path from i to j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de55e84-dd0e-41dd-bf8e-10a88613ee6e",
   "metadata": {},
   "source": [
    "### Communication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b891628b-1892-4df9-a2b6-161689db3aa9",
   "metadata": {},
   "source": [
    "i and j communicate (i <-> j) if i -> j and j -> i. Communication is an equivalence relation.\n",
    "\n",
    "Proof that if i <-> j and j <-> k, then i <-> k (transitivity):\n",
    "\n",
    "If i -> j and j -> k, then there exist an n and m such that $P^n_{ij}>0, P^m_{jk}>0$.\n",
    "\n",
    "Then, by the definition of matrix multiplication, $P^{n+m}_{ik}=\\sum_lP^n_{il}P^m_{lk}$. Then, this sum is greater than $P^n_{ij}P^m_{jk}$ (this is when l = j, so just one element of the sum), which is greater than 0. So, i -> k (because you can get from i to k in n+m steps). The proof for the reverse is similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c185e01b-90bb-4855-a999-47ba6def5548",
   "metadata": {},
   "source": [
    "### After-process notation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7e292a-21c3-4062-9bb6-0d30222e1251",
   "metadata": {},
   "source": [
    "Define $(\\theta_mX)_n=X_{m+n}$. $\\theta_mX$ is called the after m process of X. $\\theta_{\\tau}X$ is the after $\\tau$ process. $(\\theta_{\\tau}X)_n(\\omega)=X_{\\tau(\\omega)}+n(\\omega).$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc96d9d-92af-41ba-ac44-4ca2e6b99dbf",
   "metadata": {},
   "source": [
    "### Strong Markov Property with after-process notation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7386ae60-ee6a-49c1-b13a-89e2a695b003",
   "metadata": {},
   "source": [
    "$E_{x_0}[1_{\\tau<\\infty}g_{\\tau}(x_0, ... x_{\\tau})h(\\theta_{\\tau}X)]$\n",
    "\n",
    "$=E_{x_0}[1_{\\tau<\\infty}g_{\\tau}(x_0...x_{\\tau})E_{X_\\tau}[h(X)]]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ce2523-91f7-4670-af55-50af7bdbae4a",
   "metadata": {},
   "source": [
    "## Hitting time properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f934c10-52fe-4fd2-9682-6985bdf682fe",
   "metadata": {},
   "source": [
    "Let $f_{ij}=P_i(T_j^1<\\infty)$. This corresponds to the probability that state j is reachable when starting from state i at least once, as the first hitting time of j is less than infinity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf09fd2f-f0f0-49eb-9298-b9f9ca7c8271",
   "metadata": {},
   "source": [
    "Property: $P_i(T_j^k<\\infty)=f_{ij}f_{jj}^{k-1}$. This corresponds to the probability that state j is reachable starting from state i at least k times, as the kth hitting time of j is less than infinity. The property states that this probability is equivalent to the product of the probability that j is reachable at least once starting from i and the probability that j is reachable starting from state j k-1 times. Intuitively, this means that you need to get from i to j once, and then from j back to j k-1 times, in order to hit j a total of k times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a76b7a9-222b-4c56-9c3b-c8eca6d4d0a5",
   "metadata": {},
   "source": [
    "Proof by induction:\n",
    "\n",
    "For k = 1, $P_i(T_j^1<\\infty)=f_{ij}$ by definition. Assume this holds true for k.\n",
    "\n",
    "Use the expectation of an indicator variable to rewrite $P_i(T_j^{k+1}<\\infty)$.\n",
    "\n",
    "$P_i(T_j^{k+1}<\\infty)=E_i[1_{T_j^{k+1}<\\infty}]$\n",
    "\n",
    "Divide $1_{T_j^{k+1}<\\infty}$ into two conditions: $1_{T_j^k<\\infty}(X)$ and $1_{T_j^1<\\infty}(X_{T_j^k+1}, X_{T_j^k+2}...)$. The first condition corresponds to hitting j k times. The second condition corresponds to hitting j one time after having already hit it k times. Then, the indicator variable overall equals 0 if either condition is not met, and 1 if both are met, so it is equivalent to the original.\n",
    "\n",
    "$=E_i[1_{T_j^k<\\infty}(X)1_{T_j^1<\\infty}(X_{T_j^k+1}, X_{T_j^k+2}...)]$\n",
    "\n",
    "Rewrite this to use after-hitting-time notation:\n",
    "\n",
    "$=E_i[1_{T_j^k<\\infty}(X)1_{T_j^1<\\infty}(\\theta_{T_j^k}X)]$\n",
    "\n",
    "Now, it is possible to apply the Strong Markov Property:\n",
    "\n",
    "$=E_i[1_{T_j^k<\\infty}(X)E_{T_j^k}[1_{T_j^1<\\infty}(X)]]$\n",
    "\n",
    "Then, we know that the state of $T_j^k$ is j (since this is the kth hitting time of j itself), so:\n",
    "\n",
    "$=E_i[1_{T_j^k<\\infty}(X)E_{j}[1_{T_j^1<\\infty}(X)]]$\n",
    "\n",
    "Then, the expectation starting from j is a constant, so:\n",
    "\n",
    "$=E_i[1_{T_j^k<\\infty}(X)]E_{j}[1_{T_j^1<\\infty}(X)]$\n",
    "\n",
    "By definition, $E_{j}[1_{T_j^1<\\infty}(X)]=f_{jj}$, so:\n",
    "\n",
    "$=E_i[1_{T_j^k<\\infty}(X)]f_{jj}$\n",
    "\n",
    "Apply the inductive hypothesis:\n",
    "\n",
    "$=f_{ij}f_{jj}^{k-1}f_{jj}=f_{ij}f_{jj}^{k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6086eab-d9e9-45c8-acff-228e0a5802ac",
   "metadata": {},
   "source": [
    "## Classification of states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c69386-f0fc-4271-8f35-ad0f78218386",
   "metadata": {},
   "source": [
    "### Recurrence and transience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823ff76f-6b6b-457b-80e9-d2b7e7d96373",
   "metadata": {},
   "source": [
    "Let $f_{ij}=P_i(X_n=j,$ for some $n \\geq 1)$. This is the probability of ever hitting j after starting from i. A state i is recurrent if $f_{ii}=1$, and transient otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3ff39c-396f-432b-9a26-f1d59b86c2a4",
   "metadata": {},
   "source": [
    "Properties:\n",
    "\n",
    "a) i is recurrent iff $P_i(X_n=i$ for $\\infty$ many n) = 1\n",
    "\n",
    "Proof: by definition, if i is recurrent, then $f_{ii}=1$ (that is, it is almost surely certain that you return to i when starting from i).\n",
    "\n",
    "Then, $P_i(T_i^k <\\infty)=f_{ii}^k=1$ for any k (first equality comes from previous proof, second equality is from raising 1 to any power)\n",
    "\n",
    "Then, consider $P_{i}(\\cap_{k=1} T_i^k<\\infty)$. The probability is decreasing as k is increasing, so it equals $lim_{k\\rightarrow \\infty}P_i(T_i^k<\\infty)$, which is 1 for any k as stated above. This implies that $P_i(X_n=i$ for $\\infty$ many n) = 1. Converse proof is similar.\n",
    "\n",
    "b) i is transient iff $P_i(X_n=i$ for finite many n $)=1$.\n",
    "\n",
    "c) let $N_i=\\sum_{n=0}^{\\infty}1_i(X_n)$, which is the number of visits to i for n >= 0. i is recurrent iff $N_i=\\infty$, otherwise i is transient and $N_i~Geom(1-f_{ii})$.\n",
    "\n",
    "Proof of geometric distribution:\n",
    "\n",
    "$P_i(N_i=k)=P_i(T_i^{k-1}<\\infty \\cap T_i^k=\\infty)$\n",
    "\n",
    "= $E_i[1_{T_i^{k-1}<\\infty}(X)1_{T_i^1=\\infty}(\\theta_{T_i^{k-1}}(X))]$\n",
    "\n",
    "By Strong Markov property:\n",
    "\n",
    "= $E_i[1_{T_i^{k-1}<\\infty}(X)E_{X_{T_i^{k-1}}}[1_{T_i^1=\\infty}(X)]]$\n",
    "\n",
    "= $E_i[1_{T_i^{k-1}<\\infty}(X)](1-f_{ii})$\n",
    "\n",
    "= $f_{ii}^{k-1}(1-f_{ii})$, which is a geometric distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23e18b4-5c81-4445-969d-f9e9e2a93dd5",
   "metadata": {},
   "source": [
    "Proposition 4.2.3\n",
    "\n",
    "a) $E_i(N_i)=\\frac{1}{1-f_{ii}}$ is infinity if i is recurrent, less than infinity otherwise.\n",
    "\n",
    "Proofs follow from previous.\n",
    "\n",
    "b) $E_i(N_i)=\\sum_{i=0}^{\\infty}P_{ii}^n$\n",
    "\n",
    "Proof: $E_i(N_i)=E_i[\\sum_{n=0}^{\\infty}1_i(X_n)]=\\sum_{n=0}^{\\infty} E_i[1_i(X_n)]$\n",
    "\n",
    "Then, the expectation term is the expectation of a Bernoulli random variable, which is p:\n",
    "\n",
    "$=\\sum_{n=0}^{\\infty} P^n_{ii}$\n",
    "\n",
    "Green's function: $g(i, j) = \\sum_{n=1}^{\\infty}p^{(n)}_{ij}$\n",
    "\n",
    "i is recurrent iff $g(i, i)=\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09152a75-7bfc-4524-aa6d-fc030d9528a5",
   "metadata": {},
   "source": [
    "### Recurrence is a class property"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d740a665-48b2-4ceb-a6f2-41b2a75b2e9f",
   "metadata": {},
   "source": [
    "Proposition 4.2.4: If i is recurrent and $f_{ij}>0$ (so $i \\rightarrow j$), then j is also recurrent, and $f_{ij}=1$ and $f_{ji}=1$.\n",
    "\n",
    "Proof:\n",
    "\n",
    "Step 1:\n",
    "\n",
    "$i \\rightarrow j$, so $P_i(T_j^1<\\infty)>0$. i is recurrent, so $P_i(X_n=i$ for $\\infty$ many n)=1.\n",
    "\n",
    "So, $0<P_i(T_j^1<\\infty \\cap X_n=i$ for $\\infty$ many n) (because the second condition is sure, so intersecting doesn't change the probability)\n",
    "\n",
    "$ \\leq P_i(T_j^1<\\infty \\cap $ there exists an n such that $n>T_j^1$ and $X_n=i$) \n",
    "\n",
    "Transform probability to expectation of indicator variables:\n",
    "\n",
    "$ = E_i[1_{T_j^1<\\infty}1_{T_i^1<\\infty}(\\theta_{T_j}X)]$\n",
    "\n",
    "Apply Strong Markov probability:\n",
    "\n",
    "$ = E_i[1_{T_j^1<\\infty}E_{j}1_{T_i^1<\\infty}]]$\n",
    "\n",
    "$=f_{ij}f_{ji}$\n",
    "\n",
    "This quantity is greater than 0, and greater than $f_{ij}$.\n",
    "\n",
    "So $0<f_{ij}\\leq f_{ji}f_{ij}$. So, $f_{ji}=1$\n",
    "\n",
    "Step 2: show that j is recurrent through Green's function.\n",
    "\n",
    "$f_{ij}>0, f_{ji}=1>0,$ so there exist an n and m such that $P^n_{ij}>0, P^m_{ji}>0$.\n",
    "\n",
    "Construct a path from j to j passing through i of length k.\n",
    "\n",
    "Then, $\\sum_{k=0}^{\\infty}P^{(n+k+m)}_{jj}$ represents the number of visits of length n+k+m to j.\n",
    "\n",
    "$\\sum_{k=0}^{\\infty}P^{(n+k+m)}_{jj} \\geq \\sum_{k=0}^{\\infty}P^m_{ji}P^k_{ii}P^n_{ij}$ (getting from j to i in m steps, then going back to i in k steps, then from i to j in n steps). \n",
    "\n",
    "i is recurrent so the sum in the middle is infinite, so the entire sum is infinite. Thus, j is recurrent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf9cafe-4eed-418c-ae67-9253963e2ea1",
   "metadata": {},
   "source": [
    "For a finite state Markov chain, there exists at least 1 recurrent state.\n",
    "\n",
    "Proof:\n",
    "\n",
    "The Markov chain continues forever, so the total number of times all the states are hit is infinity.\n",
    "\n",
    "Then, \n",
    "\n",
    "$\\infty=\\sum_{j \\in S}N_j=E_i(\\sum_{j\\in S}N_j)=E_i[N_i]+\\sum_{j\\neq i}E_i[N_j]$\n",
    "\n",
    "$=E_i[N_i]+\\sum_{j\\neq i}E_i[1_{T_j^1<\\infty}(1 + N_j(\\theta_{T_j^1}(X))]$\n",
    "\n",
    "By Strong Markov property:\n",
    "\n",
    "$=E_i[N_i]+\\sum_{j\\neq i}E_i[1_{T_j^1<\\infty}E_{X_{T_{j}^1}}[1+N_j]]$\n",
    "\n",
    "$=E_i[N_i]+\\sum_{j\\neq i}E_i[1_{T_j^1<\\infty}E_{X_{T_{j}^1}}[N_j]]+\\sum_{j\\neq i}f_{ij}$\n",
    "\n",
    "= $E_i[g(i,i)]+\\sum_{j\\neq i}f_{ij}g(j,j)+\\sum_{j\\neq i}f_{ij}$\n",
    "\n",
    "The last sum is finite because there are a finite number of states. So, there exists at least one j such that g(j, j) is infinite, so at least one state is recurrent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
